{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fea6d23c616b470",
   "metadata": {},
   "source": [
    "# Run Moatless Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9574c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = \"alignment-data-generation\"\n",
    "base_url = \"https://api.01ww.xyz/v1\"\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "os.environ['OPENAI_API_BASE'] = base_url\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c80c1cba7d5c37",
   "metadata": {},
   "source": [
    "First, index the codebase in a vector store.\n",
    "\n",
    "Set `repo_dir` to the path of the repository you want to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dd5259592cc65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:14:44.351498Z",
     "start_time": "2024-06-17T05:14:22.983524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23476022e88a469b954f69ad888f752e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae4f379417241f88d336215e1ca2539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 177 nodes and 58057 tokens\n"
     ]
    }
   ],
   "source": [
    "from moatless.index import CodeIndex, IndexSettings\n",
    "from moatless import FileRepository, Workspace\n",
    "\n",
    "# An OPENAI_API_KEY is required to use the OpenAI Models\n",
    "model = \"gpt-4o-2024-05-13\"\n",
    "index_settings = IndexSettings(\n",
    "    embed_model=\"text-embedding-3-small\"\n",
    "    # embed_model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "repo_dir = \"/home/yifan/Code/test_repo\"\n",
    "file_repo = FileRepository(repo_path=repo_dir)\n",
    "\n",
    "code_index = CodeIndex(file_repo=file_repo, settings=index_settings)\n",
    "nodes, tokens = code_index.run_ingestion()\n",
    "\n",
    "print(f\"Indexed {nodes} nodes and {tokens} tokens\")\n",
    "\n",
    "workspace = Workspace(file_repo=file_repo, code_index=code_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ab06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='Found 1 hits.' hits=[SearchCodeHit(file_path='moatless/file_context.py', spans=[SpanHit(span_id='ContextSpan', rank=0, tokens=17), SpanHit(span_id='ContextSpan.span_id', rank=0, tokens=13), SpanHit(span_id='ContextSpan.dict', rank=0, tokens=14), SpanHit(span_id='ContextSpan.model_dump', rank=0, tokens=23)])]\n"
     ]
    }
   ],
   "source": [
    "res = code_index.find_by_name(class_names=[\"ContextSpan\"])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13c37fd492267f",
   "metadata": {},
   "source": [
    "Then use the `SearchLoop` to find the relevant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10854b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moatless.transitions import search_transitions\n",
    "from moatless import AgenticLoop\n",
    "import litellm\n",
    "\n",
    "litellm.set_verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7f211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger has no handlers, adding one.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"Loop\")\n",
    "if not logger.handlers:\n",
    "    print(\"Logger has no handlers, adding one.\")\n",
    "    # 添加一个handler，例如控制台输出\n",
    "    console_handler = logging.StreamHandler()\n",
    "    logger.addHandler(console_handler)\n",
    "# for handler in logger.handlers:\n",
    "#     print(handler)\n",
    "logging.getLogger(\"Loop\").setLevel(logging.INFO)\n",
    "logger.info(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c6f7f6422053fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:14:47.217658Z",
     "start_time": "2024-06-17T05:14:44.355086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pending:0: Running transition 1. Current total cost: 0\n",
      "Initializing first state.\n",
      "Pending:0: Creating state SearchCode with params {'model': 'gpt-4o-2024-05-13', 'id': 1}\n",
      "Pending:0: Transitioning from Pending to SearchCode\n",
      "SearchCode:1: Running transition 2. Current total cost: 0\n",
      "SearchCode:1: Create completion with 2 messages\n",
      "SearchCode:1: Do completion request to gpt-4o-2024-05-13\n",
      "SearchCode:1: Received new action Search.\n",
      "SearchCode:1: Received response with trigger did_search\n",
      "SearchCode:1: Creating state IdentifyCode with params {'model': 'gpt-4o-2024-05-13', 'ranked_spans': [RankedFileSpan(file_path='moatless/llm/completion.py', span_id='completion', rank=0, tokens=230)], 'id': 2}\n",
      "SearchCode:1: Transitioning from SearchCode to IdentifyCode\n",
      "IdentifyCode:2: Running transition 3. Current total cost: 0.01679\n",
      "IdentifyCode:2: Create completion with 2 messages\n",
      "IdentifyCode:2: Do completion request to gpt-4o-2024-05-13\n",
      "IdentifyCode:2: Received new action Identify.\n",
      "IdentifyCode:2: Received response with trigger finish\n",
      "IdentifyCode:2: Creating state DecideRelevance with params {'model': 'gpt-4o-2024-05-13', 'max_iterations': 5, 'id': 3}\n",
      "IdentifyCode:2: Transitioning from IdentifyCode to DecideRelevance\n",
      "DecideRelevance:3: Running transition 4. Current total cost: 0.023755\n",
      "DecideRelevance:3: Create completion with 2 messages\n",
      "DecideRelevance:3: Do completion request to gpt-4o-2024-05-13\n",
      "DecideRelevance:3: Received new action Decision.\n",
      "DecideRelevance:3: Received response with trigger finish\n",
      "DecideRelevance:3: Creating state Finished with params {'model': 'gpt-4o-2024-05-13', 'id': 4}\n",
      "DecideRelevance:3: Transitioning from DecideRelevance to Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moatless/llm/completion.py\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "def completion(\n",
      "    model: str,\n",
      "    messages: List,\n",
      "    max_tokens: int = 1000,\n",
      "    temperature: float = 0.0,\n",
      "    trace_name: str = \"moatless-agent\",\n",
      "    stop: Optional[List[str]] = None,\n",
      "    generation_name: Optional[str] = None,\n",
      "    tools: Optional[List[Dict[str, Any]]] = None,\n",
      ") -> litellm.ModelResponse:\n",
      "    if len(messages) == 0:\n",
      "        raise ValueError(\"At least one message is required.\")\n",
      "\n",
      "    global _trace_metadata, _mock_response\n",
      "    metadata = {}\n",
      "    metadata.update(_trace_metadata)\n",
      "\n",
      "    if generation_name:\n",
      "        metadata[\"generation_name\"] = generation_name\n",
      "\n",
      "    metadata[\"trace_name\"] = trace_name\n",
      "\n",
      "    tokens = token_counter(messages=messages[-1:])\n",
      "    if tokens > Settings.max_message_tokens:\n",
      "        raise ValueError(f\"Too many tokens in the new message: {tokens}\")\n",
      "\n",
      "    response = litellm.completion(\n",
      "        model=model,\n",
      "        max_tokens=max_tokens,\n",
      "        temperature=temperature,\n",
      "        tools=tools,\n",
      "        stop=stop,\n",
      "        metadata=metadata,\n",
      "        messages=messages,\n",
      "        mock_response=_mock_response,\n",
      "    )\n",
      "\n",
      "    return response\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "instructions = \"Remove the token limit check from the completion function\"\n",
    "\n",
    "search_loop = AgenticLoop(\n",
    "    transition_rules=search_transitions(model=model), \n",
    "    workspace=workspace,\n",
    "    initial_message=instructions,\n",
    "    prompt_log_dir=\"../logs\",\n",
    ")\n",
    "\n",
    "search_response = search_loop.run()\n",
    "\n",
    "print(workspace.file_context.create_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160cc8c9c7e202e",
   "metadata": {},
   "source": [
    "Execute the `CodeLoop` to apply the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903b67c9dff5c384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:15:06.329522Z",
     "start_time": "2024-06-17T05:14:47.219845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting initial message in run is deprecated. Set in contructor.\n",
      "Pending:0: Running transition 1. Current total cost: 0\n",
      "Initializing first state.\n",
      "Using deprecated 'initial_state'. Set initial state in transition_rules instead.\n",
      "Pending:0: Creating state PlanToCode with params {'model': 'gpt-4o-2024-05-13', 'max_prompt_file_tokens': 16000, 'max_tokens_in_edit_prompt': 500, 'id': 1}\n",
      "Pending:0: Transitioning from Pending to PlanToCode\n",
      "PlanToCode:1: Running transition 2. Current total cost: 0\n",
      "PlanToCode:1: Create completion with 2 messages\n",
      "PlanToCode:1: Do completion request to gpt-4o-2024-05-13\n",
      "PlanToCode:1: Received new action ApplyChange.\n",
      "PlanToCode:1: Received response with trigger edit_code\n",
      "PlanToCode:1: Creating state EditCode with params {'model': 'gpt-4o-2024-05-13', 'instructions': 'Remove the token limit check from the completion function by deleting the lines that count the tokens and raise a ValueError if the token count exceeds the limit.', 'file_path': 'moatless/llm/completion.py', 'span_id': 'completion', 'start_line': 29, 'end_line': 66, 'id': 2}\n",
      "PlanToCode:1: Transitioning from PlanToCode to EditCode\n",
      "EditCode:2: Running transition 3. Current total cost: 0.005895000000000001\n",
      "EditCode:2: Create completion with 2 messages\n",
      "EditCode:2: Do completion request to gpt-4o-2024-05-13\n",
      "EditCode:2: Received new action Content.\n",
      "EditCode:2: Received response with trigger finish\n",
      "EditCode:2: Creating state PlanToCode with params {'model': 'gpt-4o-2024-05-13', 'max_prompt_file_tokens': 16000, 'max_tokens_in_edit_prompt': 500, 'message': 'Applied the change to moatless/llm/completion.py.', 'diff': '--- moatless/llm/completion.py\\n+++ moatless/llm/completion.py\\n@@ -48,10 +48,6 @@\\n \\n     metadata[\"trace_name\"] = trace_name\\n \\n-    tokens = token_counter(messages=messages[-1:])\\n-    if tokens > Settings.max_message_tokens:\\n-        raise ValueError(f\"Too many tokens in the new message: {tokens}\")\\n-\\n     response = litellm.completion(\\n         model=model,\\n         max_tokens=max_tokens,\\n', 'verification_errors': [], 'id': 3}\n",
      "EditCode:2: Transitioning from EditCode to PlanToCode\n",
      "PlanToCode:3: Running transition 4. Current total cost: 0.014580000000000003\n",
      "PlanToCode:3: Create completion with 4 messages\n",
      "PlanToCode:3: Do completion request to gpt-4o-2024-05-13\n",
      "PlanToCode:3: Received new action ApplyChange.\n",
      "PlanToCode:3: Received response with trigger finish\n",
      "PlanToCode:3: Creating state Finished with params {'model': 'gpt-4o-2024-05-13', 'message': 'The token limit check has been successfully removed from the completion function. The issue has been resolved.', 'id': 4}\n",
      "PlanToCode:3: Transitioning from PlanToCode to Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The token limit check has been successfully removed from the completion function. The issue has been resolved.\n"
     ]
    }
   ],
   "source": [
    "from moatless.transitions import code_transitions\n",
    "\n",
    "litellm.set_verbose=False\n",
    "\n",
    "code_loop = AgenticLoop(\n",
    "    transition_rules=code_transitions(model=model),\n",
    "    workspace=workspace,\n",
    "    prompt_log_dir=\"../logs\",\n",
    ")\n",
    "code_response = code_loop.run(instructions)\n",
    "\n",
    "print(f\"Response: {code_response.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d131ca3793b26a",
   "metadata": {},
   "source": [
    "Run a `$ git diff` to see the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51ba9eceb0b7288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:15:06.434193Z",
     "start_time": "2024-06-17T05:15:06.332771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/moatless/llm/completion.py b/moatless/llm/completion.py\n",
      "index 2c95e47..a926948 100644\n",
      "--- a/moatless/llm/completion.py\n",
      "+++ b/moatless/llm/completion.py\n",
      "@@ -48,10 +48,6 @@ def completion(\n",
      " \n",
      "     metadata[\"trace_name\"] = trace_name\n",
      " \n",
      "-    tokens = token_counter(messages=messages[-1:])\n",
      "-    if tokens > Settings.max_message_tokens:\n",
      "-        raise ValueError(f\"Too many tokens in the new message: {tokens}\")\n",
      "-\n",
      "     response = litellm.completion(\n",
      "         model=model,\n",
      "         max_tokens=max_tokens,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.run(\n",
    "      [\"git\", \"diff\"],\n",
    "      capture_output=True,\n",
    "      text=True,\n",
    "      cwd=repo_dir,\n",
    ")\n",
    "\n",
    "print(output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d9d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
